\documentclass[10pt]{article} 
\usepackage[margin=0.8in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref} 
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{graphicx}

\input{rust-listings.tex}

\title{GxHash: A High-Throughput, Non-Cryptographic Hashing Algorithm Leveraging Modern CPU Capabilities}
\author{Olivier Giniaux}
\date{}

\begin{document}

\maketitle

\begin{abstract}

In the rapidly evolving landscape of data processing and cybersecurity, hashing algorithms play a pivotal role in ensuring data integrity and security.
Traditional hashing methods, while effective, often fail to fully utilize the computational capabilities of modern processors.
This paper introduces the GxHash hashing algorithm, a novel approach that harnesses the power of high instruction-level parallelism (ILP) and Single
Instruction, Multiple Data (SIMD) capabilities of contemporary CPUs to achieve high-throughput non-cryptographic hashing. Through a comprehensive
analysis, including benchmarks and comparisons with existing methods, we demonstrate that GxHash significantly outperforms conventional algorithms
in terms of speed and computational efficiency without compromising on security. The paper also explores the implications, limitations, and avenues
for future research in this burgeoning field.

\end{abstract}

\begin{multicols}{2}
\tableofcontents
\end{multicols}

\clearpage
\section{Introduction}

\subsection{Motivations}

As a software engineer at Equativ, a company specializing in high-performance AdServing backends that handle billions of auctions daily,
I face unique challenges in maximizing throughput while minimizing latency. In this high-stakes environment, every millisecond counts,
and the performance of underlying data structures becomes critically important. We heavily rely on in-memory caches and other hash-based data structures,
making the efficiency of hashing algorithms a non-trivial concern in our system's overall performance.\\\\
While diving into the theory of hashing out of both necessity and intellectual curiosity, I discovered that existing hashing algorithms,
including those built on well-known constructions like Merkle–Damgård, are not optimized to exploit the full capabilities of modern general-purpose CPUs.
These CPUs offer advanced features such as Single Instruction, Multiple Data (SIMD) and Instruction-Level Parallelism (ILP), which remain largely untapped
by current hashing methods.\\\\
The challenge of creating a faster, more efficient hashing algorithm became not just a professional necessity but also a personal quest.
It was both challenging and exhilarating to delve into hashing theory and experiment with new approaches. The result is what I believe to be the fastest
non-cryptographic hashing algorithm developed to date.\\\\
The primary motivation behind this research is to bridge the existing performance gap by designing a hashing algorithm that fully leverages SIMD and ILP
capabilities. The aim is to achieve an order-of-magnitude improvement in hashing speed, thereby revolutionizing the efficiency of various applications,
from databases to real-time analytics and beyond.\\\\
In summary, this work is driven by both the practical needs of my professional environment and a personal passion for pushing the boundaries of what is
technically possible in the realm of hashing algorithms.

\subsection{Scope Limitation}

This paper primarily investigates methods for enhancing the throughput of non-cryptographic hashing algorithms,
which are commonly used in a variety of time-sensitive applications, including but not limited to hashmap lookups.
In such contexts, hash computation is often expected to be extremely fast, typically requiring execution times ranging
from nanoseconds to microseconds. Given these considerations, the paper intentionally excludes Thread-Level Parallelism
(TLP) as a viable strategy for performance optimization.\\\\
Firstly, it's worth mentioning that TLP is orthogonal to the methods being explored in this paper.
TLP focuses on optimizing coarser-grained operations by distributing tasks across multiple threads,
which doesn't directly align with the fine-grained performance improvements we aim to achieve.\\\\
Secondly, introducing TLP would necessitate the incorporation of task scheduling, synchronization, and context switching.
These overheads, while perhaps manageable in applications that can afford greater latencies, become less justifiable when
aiming for high-throughput, low-latency operations that are typical in non-cryptographic hashing scenarios.\\\\
In summary, while TLP might be a suitable performance optimization strategy in other computational contexts, we chose to
leave it aside in this paper.

\subsubsection{Portability}

\clearpage
\section{Related Work}

The field of hashing algorithms has been a subject of extensive research and development, tracing its roots back to foundational architectures like
the \textbf{Merkle–Damgård} Construction\cite{merkle}\cite{damgard}, introduced in 1989. Over the years, this area has seen a plethora of innovations, each attempting to address various
aspects of hashing—be it collision resistance, distribution uniformity, or computational efficiency. While cryptographic hashing has often been the focal
point of research, non-cryptographic hashing algorithms have also garnered attention for their utility in data structures like hashmaps and caches.

In this section, we will explore some of the seminal works and recent advancements in the realm of hashing algorithms to contextualize
our research.

\subsection{The Merkle--Damgård Construction}

The Merkle–Damgård construction serves as the foundational architecture for many existing hash functions.
It operates by breaking down an input into fixed-size blocks, which are then processed sequentially through a compression
function. The output of each block feeds into the next, culminating in a final, fixed-size hash.\\\\
To formalize it, let us denote a hash function \( h: \{0,1\}^{n_b \times s_b} \to \{0,1\}^{s_h} \), where:
\begin{itemize}
    \item \( M \) represents an input message that is divided into \( n_b \) blocks, each of \( s_b \) bits. In formal notation, \( M = M_1 \parallel M_2 \parallel \cdots \parallel M_{n_b} \).
    \item The output is a hash value with a fixed bit-length \( s_h \).
\end{itemize}
Before proceeding to the formal definition, we need to establish the key functional components that constitute the Merkle--Damgård architecture:
\begin{itemize}
    \item A compression function \( f: \{0,1\}^{s_b} \times \{0,1\}^{s_b} \to \{0,1\}^{s_b} \),
    \item A finalization function \( g: \{0,1\}^{s_b} \to \{0,1\}^{s_h} \),
    \item An initialization vector \( 0^{s_b} \), comprised of \( s_b \) zero bits,
\end{itemize}

With these components, the hash function \( h \) may be articulated as follows:
\begin{align*}
    h(M) &= g\left( f(\cdots f(f(0^{s_b}, M_1), M_2) \cdots, M_{n_b}) \right)
\end{align*}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{linear-construction.png}
\caption{Merkle–Damgård Construction Overview}
\label{fig:linear-construction}
\end{figure}

\subsection{The Wide-Pipe Construction Variant} \label{widepipe}

The \textbf{Wide-Pipe} construction serves as a variant of the traditional Merkle–Damgård architecture and aims to improve the hash
function's cryptographic resilience.\\\\
The standard Merkle–Damgård construction has been found vulnerable to certain types of cryptographic attacks, including length extension
and multicollision attacks. The Wide-Pipe variant mitigates these vulnerabilities by modifying the hash function's internal state.
In this construction, the size of the internal state (\(s_b\)) is deliberately made much larger than the size of the hash output
(\(s_h\)), as expressed by the inequality:
\begin{align*}
    s_b \gg s_h
\end{align*}

This design choice serves to complicate potential attack vectors by introducing a greater level of computational complexity.
As a result, the hash function gains increased resistance against certain types of attacks that exploit the limitations of the
Merkle–Damgård construction.\\\\
While performance remains our primary focus, the algorithm we examine in this paper incorporates the Wide-Pipe idea. By doing so,
it offers a dual advantage: performance efficacy, which is central to this study, along with a good cryptographic resilience,
which is an indeniable advantage even in the realm of non-cryptographic hash functions.

\subsection{Other Variants}

While the classical Merkle–Damgård construction provides a reliable framework for cryptographic hash functions,
there are a wide number of variants\cite{merkle_damgard_alternatives_review}. In this section, we discuss around some
of the most popular variants around opportunities for performance enhancement.

\subsubsection{Sponge Construction}

The Sponge construction,  proceeds in two phases: absorption and squeezing.
During the absorption phase, blocks of input data are XORed into a portion of the internal state,
followed by a cryptographic permutation of the entire state. Importantly, each absorption step is inherently sequential,
as it relies on the state resulting from the cryptographic permutation of the previous step. Consequently,
the algorithm does cannot benefit from instruction-level parallelism during the absorption phase.
The squeezing phase, which follows the absorption of all input blocks, generates the output hash from the internal state.
The opportunity for parallelization with the Sponge construction is thus constrained by its sequential nature during the absorbing phase.

\subsubsection{HAIFA Construction}

The HAsh Iterative FrAmework (HAIFA) not only mitigates vulnerabilities but also incorporates features like support for a configurable number of rounds and real-time incremental hashing.
However, like the standard Merkle–Damgård and Sponge constructions, the HAsh Iterative FrAmework (HAIFA) is also inherently sequential in its processing of message blocks.
Each iteration of the compression function in HAIFA depends on the outcome of the previous iteration.
This dependency chain means that the construction does not naturally lend itself to instruction-level parallelism (ILP),
although other types of optimizations may still be possible depending on the specific implementation and hardware architecture.

\subsubsection{Tree Hashing}

Tree hashing breaks the input message into smaller fragments and processes them independently in a tree-like structure.
While tree-based hashing constructions offer a degree of parallelism, it is essential to recognize their inherent limitations. The extent of parallelization diminishes progressively as we move upward through the tree, owing to the dependencies between higher-level nodes. Here's a breakdown:

\begin{itemize}
\item \textbf{Leaf Level:} At this level, all blocks are independent, allowing the hashing operations to occur in parallel. If \( n \) is the number of leaves, then \( \frac{n}{2} \) parallel operations can occur for an even \( n \), or \( \frac{n-1}{2} + 1 \) for an odd \( n \).
\item \textbf{Second Level and Above:} The second level requires the hash results from the first level. Each node at the second level depends on two nodes from the level below, effectively halving the potential parallelism. This pattern of diminishing parallelization continues in subsequent levels.
\item \textbf{Final Level:} At the top of the tree, we are left with a single hash that relies on the two hashes beneath it. This operation is inherently sequential.
\end{itemize}

Although the tree-based hashing approach has a theoretical advantage in parallelization, practical implementations often face several challenges that can impact efficiency. These issues can make tree hashing hardly as efficient as a sequential structure for certain applications. Here are the primary factors:

\begin{itemize}
\item \textbf{Synchronization Overhead:} The parallel nature of the algorithm necessitates synchronization between different processing threads or units, especially at higher tree levels where dependencies exist. This overhead counters the gains from parallelization.
\item \textbf{Memory Consumption:} Tree constructions typically require more memory to store intermediate hash values, particularly when branching factors are high. Memory allocation and fragmentation usually impact performance.
\item \textbf{Cache Efficiency:} Unlike sequential algorithms that can benefit from cache locality, the tree-based approach often has to handle multiple non-contiguous data blocks, potentially leading to cache misses and reduced efficiency.
\item \textbf{Implementation Complexity:} The algorithmic and data-structure requirements for implementing tree-based hashing are more complex than those of a straightforward sequential hashing algorithm. The increased complexity can introduce more room for errors and maintenance challenges.
\end{itemize}

In light of these practical challenges, tree-based constructions might not be the best fit given our high performance goals and the architecture of today's general purpose computers.

\clearpage
\section{High ILP Construction} \label{highilp}

Most modern general-purpose CPUs employ a superscalar architecture which enables Instruction-Level Parallelism (ILP). 
Minimizing dependencies in an algorithm allows a superscalar processor to execute more instructions concurrently,
thus maximizing its inherent parallelism and overall performance.
The key limitation for ILP in the Merkle–Damgård construction is the inherent sequential dependency:
each block's hash depends on the result of hashing the previous block.

\subsection{ILP Awareness}

While compilers and CPUs employ various techniques to optimize ILP, their capabilities are often constrained by the inherent data dependencies
in the code. It's for this reason that algorithmic design can be pivotal. By structuring algorithms to minimize dependency chains from the outset,
we create opportunities for higher ILP that even the most advanced compiler optimizations and CPU features cannot achieve alone. Therefore,
algorithmic design that is mindful of ILP can be a game-changer for performance optimization.

\subsubsection{Example}
Let's take a FNV-like hashing function. The "naive" way to process an array of elements would look like the \texttt{baseline} method,
as shown in Rust snippet (Figure~\ref{fig:ilp_rust_example}). As we can see, every loop iteration requires the value of \texttt{h},
which has been computed the iteration before. Here we have a dependency chain, preventing the compiler from doing any optimization.

To make ILP possible, for the function \texttt{temp} we unroll the loop and hash a few inputs altogether, independently from \texttt{h},
and mix it once thereafter with \texttt{h}. We still have a dependency chain on \texttt{h}, but for less iterations.
The temporary hashes are independent and thus eligible for ILP.

Another track taken for the function \texttt{laned} is to unroll the loop and hash on separate lanes,
and then mix the lanes together upon exiting the loop. Each lane has its own dependency chain, but on less iterations.

\begin{figure}[ht]
\begin{multicols}{2}
\begin{lstlisting}[language=Rust, style=boxed]
const PRIME: u64 = 0x00000100000001b3;
const OFFSET: u64 = 0xcbf29ce484222325;

#[inline]
fn hash(hash: u64, value: u64) -> u64 {
    (hash ^ value) * PRIME
}

fn baseline(input: &[u64]) -> u64 {
    let mut h = OFFSET;
    let mut i: usize = 0;
    while i < input.len() {
        h = hash(h, input[i]);

        i = i + 1;
    }
    h
}

fn unrolled(input: &[u64]) -> u64 {
    let mut h: u64 = OFFSET;
    let mut i: usize = 0;
    while i < input.len() {
        h = hash(h, input[i]);
        h = hash(h, input[i + 1]);
        h = hash(h, input[i + 2]);
        h = hash(h, input[i + 3]);
        h = hash(h, input[i + 4]);

        i = i + 5;
    }
    h
}
\end{lstlisting}
\columnbreak
\begin{lstlisting}[language=Rust, style=boxed]
fn temp(input: &[u64]) -> u64 {
    let mut h: u64 = OFFSET;
    let mut i: usize = 0;
    while i < input.len() {
        let mut tmp: u64 = input[i];
        tmp = hash(tmp, input[i + 1]);
        tmp = hash(tmp, input[i + 2]);
        tmp = hash(tmp, input[i + 3]);
        tmp = hash(tmp, input[i + 4]);

        h = hash(h, tmp);

        i = i + 5;
    }
    h
}

fn laned(input: &[u64]) -> u64 {
    let mut h1: u64 = OFFSET;
    let mut h2: u64 = OFFSET;
    let mut h3: u64 = OFFSET;
    let mut h4: u64 = OFFSET;
    let mut h5: u64 = OFFSET; 
    let mut i: usize = 0;
    while i < input.len() {
        h1 = hash(h1, input[i]);
        h2 = hash(h2, input[i + 1]);
        h3 = hash(h3, input[i + 2]);
        h4 = hash(h4, input[i + 3]);
        h5 = hash(h5, input[i + 4]);

        i = i + 5;
    }
    hash(hash(hash(hash(h1, h2), h3), h4), h5)
}
\end{lstlisting}
\end{multicols}
\caption{FNV-like hash functions in Rust}
\label{fig:ilp_rust_example}
\end{figure}

\clearpage
\subsubsection{Benchmark}
Here are the timing on both an x86 and an ARM CPU. It also includes timing for the function \texttt{unrolled},
to show that performance increase comes indeed from ILP and not the loop unrolling itself.
We can see that \texttt{temp} and \texttt{laned} performed equally, leveraging ILP for a significant performance increase
over the \texttt{baseline}.
\\
\begin{figure}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
CPU & \texttt{baseline} & \texttt{unrolled} & \texttt{temp} & \texttt{laned} \\
\hline
AMD Ryzen 5 5625U (x86 64-bit) & 92.787 µs & 93.047 µs & 37.516 µs & 37.434 µs \\
Apple M1 Pro (ARM 64-bit) & 125.23 µs & 124.42 µs & 28.507 µs & 30.716 µs \\
\hline
\end{tabular}
\caption{Benchmark timings for ILP example}
\label{tab:your_table_label}
\end{figure}
\\
While the \texttt{temp} and \texttt{laned} functions won't yield exactly the same hashes as the \texttt{baseline},
they serve the same purpose, while being much faster.
Both approaches have they pro and cons. As the \texttt{laned} explicitely declares \( n \) variables for our lanes,
this approach is simpler in regards to compiler analysis and is thus more likely to benefit from ILP,
regardless of the compiler or programming language used.
The following section will delve more in depth onto the definition of a \textbf{Laned Construction}.

\clearpage
\subsection{The Laned Construction}

The \textbf{Laned Construction} introduces \( k_b \) lanes, and processes the message by groups of \( k_b \) blocks,
so that for a given group each of the \( k_b \) block and be processed on its own lane.
This way, we have \( k_b \) independent dependency chains. The lane hashes can then be compressed altogether thereafter upon
exiting the loop.

\subsubsection{Intermediate Hashes}

Let's define \( n_g = \lfloor {n_b}/{k_b} \rfloor \) as the number of whole groups of \( k_b \) message blocks. \\
For each lane we compute an intermediate hash, \( H_i \), as follows:

\begin{align*}
H_{1} &= f(\ldots f(f(f(0^{s_b}, M_{0k_b + 1}), M_{1k_b + 1}), M_{2k_b + 1})\ldots, M_{n_g + 1}) \\
H_{2} &= f(\ldots f(f(f(0^{s_b}, M_{0k_b + 2}), M_{1k_b + 2}), M_{2k_b + 2})\ldots, M_{n_g + 2}) \\
&\vdots \\
H_{k_b} &= f(\ldots f(f(f(0^{s_b}, M_{0k_b + k_b}), M_{1k_b + k_b}), M_{2k_b + k_b})\ldots, M_{n_g + k_b}) \\
\end{align*}

\subsubsection{Final Hash}

The final hash \( H \) is calculated using \( f \) to compress the intermediate hashes and the remaining message blocks (if any),
which is then passed through \( g \):

\begin{equation*}
h(M) = g\left( f( \ldots f(f(\ldots f(f(0^{s_b}, H_1), H_2) \ldots, H_{k_b}), M_{{k_b}{n_g}+1}) \ldots, M_{n_b} ) \right).
\end{equation*}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{laned-construction.png}
\caption{Laned Construction Overview}
\label{fig:linear-construction}
\end{figure}

\section{The GxHash Algorithm}

The \textbf{GxHash} hashing algorithm employs an high ILP construction as described in section \ref{highilp}. In practice,
this construction not only enables instruction level paralellism but also enables loop unrolling, disminishing the looping overhead,
thus maximizing computational efficiency and overall performance.\\\\
With that ILP unlocked, we still have to define f and g. In our quest for high-thoughput, it is essential for us to find computationally
efficient definitions TODO

\subsection{Pipe Width}
As for the wide-pipe construction (as seen in section \ref{widepipe}) with \( s_b \) set to the executing CPU SIMD register size.
TODO

\subsection{Compression}

TODO

\subsection{Hashing}

TODO

\subsection{Folding}

TODO

\subsection{Implementation Details}

SIMD
ILP
Vectorized
Parallelized
Optimized
Throughput
Hashing

HIGH ILP CONSTRUCTION

\subsubsection{Padding}

\textbf{Merkle–Damgård} and derivatives can actually handle message of an arbitrary size \( s_m \) by padding the message upfront with the padding
function \( p: \{0,1\}^{s_m} \to \{0,1\}^{n_b \times s_b} \) where \( n_b = \lceil s_m/s_b \rceil \).

In practice, implementing \( p \) in computer code implies memory copies. Instead, TODO

\subsubsection{Low-Overhead Looping}

\subsubsection{Loop Unrolling}

\subsubsection{CPU Alignement}


\section{Benchmarks}
\subsection{Setup}
\subsection{Evaluation Metrics}
\subsection{Comparison with Existing Methods}

t1ha\cite{rust-t1ha}

\section{Discussion}
\subsection{Implications}
\subsection{Limitations}
\subsection{Future Work}

\section{Conclusion}

\bibliography{references}
\bibliographystyle{plain}

\end{document}
